{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba7a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"FALSE\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ready-robert",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:50.707488Z",
     "iopub.status.busy": "2021-04-12T07:59:50.706702Z",
     "iopub.status.idle": "2021-04-12T07:59:53.360233Z",
     "shell.execute_reply": "2021-04-12T07:59:53.359779Z"
    },
    "papermill": {
     "duration": 2.688706,
     "end_time": "2021-04-12T07:59:53.360375",
     "exception": false,
     "start_time": "2021-04-12T07:59:50.671669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax version 0.4.1\n",
      "Flax version 0.5.0\n",
      "Available devices: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import utils\n",
    "from matplotlib import pyplot as plt\n",
    "from einops import rearrange\n",
    "\n",
    "print('Jax version', jax.__version__)\n",
    "print('Flax version', flax.__version__)\n",
    "random_key = jax.random.PRNGKey(42)\n",
    "\n",
    "print(\"Available devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-empire",
   "metadata": {
    "papermill": {
     "duration": 0.030053,
     "end_time": "2021-04-12T07:59:53.421246",
     "exception": false,
     "start_time": "2021-04-12T07:59:53.391193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook contains an introduction of the paper [Glow: Generative Flow with Invertible 1Ã—1 Convolutions](https://arxiv.org/pdf/1807.03039.pdf) with an implementation in `jax`. I've also incorporated some of the \"tricks\" from the authors' original [tensorflow repository](https://github.com/openai/glow/blob/master/model.py#L559), though not all (e.g. of ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "healthy-richardson",
   "metadata": {
    "papermill": {
     "duration": 0.049064,
     "end_time": "2021-04-12T07:59:53.508299",
     "exception": false,
     "start_time": "2021-04-12T07:59:53.459235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GLOW\n",
    "Glow is based on the variational auto-encoder framework with normalizing flows. \n",
    "A normalizing flow *g* is a reversible operation with an easy to compute gradient, which allows for exact computation of the likelihood, via the chain rule equation:\n",
    "\n",
    "\\begin{align}\n",
    "x &\\leftrightarrow g_1(x) \\leftrightarrow \\dots \\leftrightarrow z\\\\\n",
    "z &= g_N \\circ \\dots \\circ g_1(x)\\\\\n",
    "\\log p(x) &= \\log p(z) + \\sum_{i=1}^N \\log \\det | \\frac{d g_{i}}{d g_{i - 1}} | \\ \\ \\ \\ \\ (1)\n",
    "\\end{align}\n",
    "\n",
    "where $x$ is the input data to model, and $z$ is the latent, as in the standard VAE framework\n",
    "\n",
    "**Note**: In the Glow setup, the architecture is fully reversible, i.e., it is only composed of normalizing flow operations, which means we can compute $p(x)$ exactly. This also implies that there is no loss of information, i.e. $z$, and the intermediate variables, have as many parameters as $x$.\n",
    "\n",
    "## Overall architecture \n",
    "Similar to [Real NVP](https://arxiv.org/abs/1605.08803), the Glow architecture has a multi-scale structure with $L$ scales, each containing $K$ iterations of  normalizing flow. Each block is separated by squeeze/split operations.\n",
    "\n",
    "![Glow_figure](https://ameroyer.github.io/images/posts/glow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-catalyst",
   "metadata": {
    "papermill": {
     "duration": 0.032077,
     "end_time": "2021-04-12T07:59:55.394969",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.362892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Learnable prior\n",
    "Remember that, in order to estimate the model likelihood $\\log p(x)$ in Equation (1), we need to compute the prior $p(z)$ on all the latent variables($z_1, \\dots, z_L$). In the original code, the prior for each $z_i$ is assumed to be Gaussian whose mean and standard deviation $\\mu_i$ and $\\sigma_i$ are learned.\n",
    "\n",
    "More specifically, after the split operation, we obtain $z_i$, the latent variable at the current scale, and $x$, the remaining output that will be propagated down the next scale. Following the original repo, we add one convolutional layer on top of $x$, to estimate the $\\mu$ and $\\sigma$ parameters of the prior $p(z_i) = \\mathcal N(\\mu, \\sigma)$. \n",
    "\n",
    "In summary, the **forward pass** (estimate the prior) becomes:\n",
    "\n",
    "\\begin{align}\n",
    "z, x &= split(x_{i})\\\\\n",
    "\\mu, \\sigma &= \\mbox{MLP}_{\\mbox{prior}}(x)\\\\\n",
    "y &= \\mbox{flow_at_scale_i}(x)\n",
    "\\end{align}\n",
    "\n",
    "and the reverse pass is \n",
    "\n",
    "\\begin{align}\n",
    "x &= \\mbox{flow_at_scale_i}^{-1}(y)\\\\\n",
    "\\mu, \\sigma &= \\mbox{MLP}_{\\mbox{prior}}(x)\\\\\n",
    "z &\\sim \\mathcal{N}(\\mu, \\sigma)\\\\\n",
    "x &= \\mbox{concat}(z, x)\n",
    "\\end{align}\n",
    "\n",
    "The MLP is initialized with all zeros weights, which corresponds to a $\\mathcal N(0, 1)$ prior. The `split` operation combines with the learnable prior becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "random-brand",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:55.471787Z",
     "iopub.status.busy": "2021-04-12T07:59:55.470757Z",
     "iopub.status.idle": "2021-04-12T07:59:55.477253Z",
     "shell.execute_reply": "2021-04-12T07:59:55.476826Z"
    },
    "papermill": {
     "duration": 0.05055,
     "end_time": "2021-04-12T07:59:55.477376",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.426826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvZeros(nn.Module):\n",
    "    features: int\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x, logscale_factor=3.0):\n",
    "        \"\"\"A simple convolutional layers initializer to all zeros\"\"\"\n",
    "        x = nn.Conv(self.features, kernel_size=(3, 3),\n",
    "                    strides=(1, 1), padding='same',\n",
    "                    kernel_init=jax.nn.initializers.zeros,\n",
    "                    bias_init=jax.nn.initializers.zeros)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Split(nn.Module):\n",
    "    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x, reverse=False, z=None, eps=None, temperature=1.0):\n",
    "        \"\"\"Args (reverse = True):\n",
    "            * z: If given, it is used instead of sampling (= deterministic mode).\n",
    "                This is only used to test the reversibility of the model.\n",
    "            * eps: If z is None and eps is given, then eps is assumed to be a \n",
    "                sample from N(0, 1) and rescaled by the mean and variance of \n",
    "                the prior. This is used during training to observe how sampling\n",
    "                from fixed latents evolve. \n",
    "               \n",
    "        If both are None, the model samples z from scratch\n",
    "        \"\"\"\n",
    "        if not reverse:\n",
    "            del z, eps, temperature\n",
    "            z, x = jnp.split(x, 2, axis=-1)\n",
    "            \n",
    "        # Learn the prior parameters for z\n",
    "        prior = ConvZeros(x.shape[-1]*2, name=\"conv_prior\")(x)\n",
    "            \n",
    "        # Reverse mode: Only return the output\n",
    "        if reverse:\n",
    "            # sample from N(0, 1) prior (inference)\n",
    "            if z is None:\n",
    "                if eps is None:\n",
    "                    eps = jax.random.normal(self.key, x.shape) \n",
    "                eps *= temperature\n",
    "                mu, logsigma = jnp.split(prior, 2, axis=-1)\n",
    "                z = eps * jnp.exp(logsigma) + mu\n",
    "            return jnp.concatenate([z, x], axis=-1)\n",
    "        # Forward mode: Also return the prior as it is used to compute the loss\n",
    "        else:\n",
    "            return z, x, prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-controversy",
   "metadata": {
    "papermill": {
     "duration": 0.031729,
     "end_time": "2021-04-12T07:59:55.540977",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.509248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Flow step\n",
    "The normalizing flow step in Glow is composed of 3 operations:\n",
    "  * `Affine Coupling Layer`: A coupling layer which splits the input data along channel dimensions, using the first half to estimate parameters of a transformation then applied to the second half (similar to [`RealNVP`](https://arxiv.org/abs/1605.08803)).\n",
    "  * `ActNorm`: Normalization layer similar to batch norm, except that the mean and standard deviation statistics are trainable parameters rather than estimated from the data (this is in particular useful here because the model sometimes has to be trained with very small batch sizes due to memory requirements)\n",
    "  * `Conv 1x1`: An invertible 1x1 convolution layer. This is a generalization of the channel permutation used in [`RealNVP`](https://arxiv.org/abs/1605.08803)\n",
    "  \n",
    "See following sections for more details on each operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-musician",
   "metadata": {
    "papermill": {
     "duration": 0.033083,
     "end_time": "2021-04-12T07:59:55.606941",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.573858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Affine Coupling (ACL)\n",
    "\n",
    "**Forward**\n",
    "\\begin{align}\n",
    "x_a, x_b &= \\mbox{split}(x)\\\\\n",
    "(\\log \\sigma, \\mu) &= \\mbox{NN}(x_b)\\\\\n",
    "y_a &= \\sigma \\odot x_a + \\mu\\\\\n",
    "y &= \\mbox{concat}(y_a, x_b)\n",
    "\\end{align}\n",
    "\n",
    "**Backward**\n",
    "\\begin{align}\n",
    "y_a, y_b &= \\mbox{split}(y)\\\\\n",
    "(\\log \\sigma, \\mu) &= \\mbox{NN}(y_b)\\\\\n",
    "x_a &= (x_a - \\mu) / \\sigma\\\\\n",
    "x &= \\mbox{concat}(x_a, y_b)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Log-det:**\n",
    "$\\log \\det \\mbox{ACL} = \\sum \\log (| \\sigma |)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "young-track",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:55.683697Z",
     "iopub.status.busy": "2021-04-12T07:59:55.682990Z",
     "iopub.status.idle": "2021-04-12T07:59:55.686650Z",
     "shell.execute_reply": "2021-04-12T07:59:55.687054Z"
    },
    "papermill": {
     "duration": 0.047811,
     "end_time": "2021-04-12T07:59:55.687194",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.639383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AffineCoupling(nn.Module):\n",
    "    out_dims: int\n",
    "    width: int = 512\n",
    "    eps: float = 1e-8\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, logdet=0, reverse=False, dilation=True, only_neighbours=True):\n",
    "        \n",
    "        ##TODO - set a seed\n",
    "        if not reverse:\n",
    "            random_patch = random.randint(0,15)\n",
    "        else:\n",
    "            random_patch = random.randint(0,3)\n",
    "        #inputs.shape = (batch_size,250,250,3) if the image has not been scaled down, otherwise it will be (batch_size,32,32,3)\n",
    "        \n",
    "        ##Get neighbours of the selected patch\n",
    "        if only_neighbours:\n",
    "            neighbours = utils.get_neighbours(random_patch)\n",
    "        \n",
    "        # We select one scaled image out of a batch, and now divide it into 16 patches. O/P dimension here = (batch_size,16,8,8,3)\n",
    "        if not dilation:\n",
    "            all_patches = rearrange(inputs, 'b (nh hp) (nw wp) c -> b (nh nw) hp wp c ', hp=8, wp=8)#Assuming that image is scaled. For 250*250, hp=wp=50 for 25 patches\n",
    "        else:\n",
    "            all_patches = rearrange(inputs, 'b (nh hp) (nw wp) c -> b (hp wp) nh nw c ', hp=4, wp=4)#dilated convolution\n",
    "        chosen_patch = all_patches[:,random_patch,:,:,:] #shape = (batch_size,8,8,3)\n",
    "        x_chosen = jnp.expand_dims(chosen_patch,axis=1) #output shape = (batch_size,1,8,8,3)\n",
    "        x_rest =  jnp.delete(all_patches,random_patch,axis=1) # (batch_size,15,8,8,3)\n",
    "        \n",
    "        ##TODO - 64x64 -with dilation 8x8 patch and 16x16 patch\n",
    "        # NN\n",
    "        # conv_module = nn.Conv(features=512,kernel_size=(3,3),strides=(1,1),padding='same')\n",
    "        # net = conv_module.init(jax.random.PRNGKey(0),x)\n",
    "        # net = conv_module.apply(net,x)\n",
    "    \n",
    "        net = nn.Conv(features=self.width, kernel_size=(3, 3), strides=(1, 1),\n",
    "                      padding='same', name=\"ACL_conv_1\")(chosen_patch) # o/p shape = (batch_size,8,8,self.width)\n",
    "        net = nn.relu(net)\n",
    "        net = nn.Conv(features=self.width, kernel_size=(1, 1), strides=(1, 1),\n",
    "                      padding='same', name=\"ACL_conv_2\")(net)\n",
    "        net = nn.relu(net)\n",
    "        net = ConvZeros((self.out_dims*2), name=\"ACL_conv_out\")(net) # o/p shape = (batch_size,8,8,6) so that the split of mu and logsigma can make the element wise multiplication possible\n",
    "        #x_net = jnp.expand_dims(net,axis=1) #output shape = (batch_size,1,8,8,6)\n",
    "        mu, logsigma = jnp.split(net, 2, axis=-1) # mu and logsigma will be also the same dimension as the to-be-transformed\n",
    "        sigma = jax.nn.sigmoid(logsigma + 2.)\n",
    "        if not only_neighbours:\n",
    "        #mu and sigma (batch_size,1,8,8,3) ->  (batch_size,15,8,8,3)\n",
    "            if not reverse:\n",
    "                #get neighbours of the chosen patch\n",
    "                mu = mu.repeat(15, axis=1)\n",
    "                sigma = sigma.repeat(15, axis=1)\n",
    "            else:\n",
    "                mu = mu.repeat(3, axis=1)\n",
    "                sigma = sigma.repeat(3, axis=1)\n",
    "            \n",
    "            yb = (x_rest - mu) / (sigma + self.eps)\n",
    "            logdet -= jnp.sum(jnp.log(sigma), axis=(1, 2, 3))\n",
    "            y = jnp.insert(arr=yb,obj=random_patch,values=x_chosen,axis=1)\n",
    "                 \n",
    "        else:\n",
    "            if not reverse:\n",
    "                for index in neighbours:\n",
    "                    all_patches = all_patches.at[:,index,:,:,:].multiply(sigma)\n",
    "                    all_patches = all_patches.at[:,index,:,:,:].add(mu)\n",
    "                logdet += jnp.sum(jnp.log(sigma), axis=(1, 2, 3))\n",
    "                y = all_patches\n",
    "            else:\n",
    "                for index in neighbours:\n",
    "                    all_patches = all_patches.at[:,index,:,:,:].add(-mu)\n",
    "                    all_patches = all_patches.at[:,index,:,:,:].divide(sigma+self.eps)\n",
    "                logdet -= jnp.sum(jnp.log(sigma), axis=(1, 2, 3))\n",
    "                y = all_patches   \n",
    "\n",
    "        #Turn patches back to the image\n",
    "        if not dilation:\n",
    "            if not reverse:\n",
    "                y = rearrange(y, 'b (nh nw) hp wp c -> b (nh hp) (nw wp) c ', nh=4, nw=4) #turn back y into (256, 32, 32, 3)\n",
    "            else:\n",
    "                y = rearrange(y, 'b (nh nw) hp wp c -> b (nh hp) (nw wp) c ', nh=2, nw=2) #turn back y into (256, 32, 32, 3)\n",
    "        else:\n",
    "            y = rearrange(y, 'b (hp wp) nh nw c -> b (nh hp) (nw wp) c ', hp=4, wp=4) #dilated convolution\n",
    "            \n",
    "        return y, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-simple",
   "metadata": {
    "papermill": {
     "duration": 0.032015,
     "end_time": "2021-04-12T07:59:55.752174",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.720159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Activation Norm\n",
    "\n",
    "**Forward**\n",
    "\\begin{align}\n",
    "y = x * \\sigma + \\mu\n",
    "\\end{align}\n",
    "\n",
    "**Backward**\n",
    "\\begin{align}\n",
    "x = (y - \\mu) / \\sigma\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Log-det:**\n",
    "$\\log \\det \\mbox{ActNorm} = h \\times w \\times \\sum \\log (| \\sigma |)$\n",
    "\n",
    "Note that $\\mu$ and $\\sigma$ are trainable variables (contrary to batch norm) and are initialized in a data-dependant manner, such that the first batch of data used for initialization is normalized to zero-mean and unit-variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "superior-groove",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:55.828269Z",
     "iopub.status.busy": "2021-04-12T07:59:55.827735Z",
     "iopub.status.idle": "2021-04-12T07:59:55.831558Z",
     "shell.execute_reply": "2021-04-12T07:59:55.831034Z"
    },
    "papermill": {
     "duration": 0.047485,
     "end_time": "2021-04-12T07:59:55.831719",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.784234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActNorm(nn.Module):\n",
    "    scale: float = 1.\n",
    "    eps: float = 1e-8\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, logdet=0, reverse=False):\n",
    "        # Data dependent initialization. Will use the values of the batch\n",
    "        # given during model.init\n",
    "        axes = tuple(i for i in range(len(inputs.shape) - 1))\n",
    "        def dd_mean_initializer(key, shape):\n",
    "            \"\"\"Data-dependant init for mu\"\"\"\n",
    "            nonlocal inputs\n",
    "            x_mean = jnp.mean(inputs, axis=axes, keepdims=True)\n",
    "            return - x_mean\n",
    "        \n",
    "        def dd_stddev_initializer(key, shape):\n",
    "            \"\"\"Data-dependant init for sigma\"\"\"\n",
    "            nonlocal inputs\n",
    "            x_var = jnp.mean(inputs**2, axis=axes, keepdims=True)\n",
    "            var = self.scale / (jnp.sqrt(x_var) + self.eps)\n",
    "            return var\n",
    "        \n",
    "        # Forward\n",
    "        shape = (1,) * len(axes) + (inputs.shape[-1],)\n",
    "        mu = self.param('actnorm_mean', dd_mean_initializer, shape)\n",
    "        sigma = self.param('actnorm_sigma', dd_stddev_initializer, shape)\n",
    "        \n",
    "        logsigma = jnp.log(jnp.abs(sigma))\n",
    "        # logdet_factor = reduce(\n",
    "        #     operator.mul, (inputs.shape[i] for i in range(1, len(inputs.shape) - 1)), 1)\n",
    "        logdet_factor = 1\n",
    "        if not reverse:\n",
    "            y = sigma * (inputs + mu)\n",
    "            logdet += logdet_factor * jnp.sum(logsigma)\n",
    "        else:\n",
    "            y = inputs / (sigma + self.eps) - mu\n",
    "            logdet -= logdet_factor * jnp.sum(logsigma)\n",
    "        \n",
    "        # Logdet and return\n",
    "        return y, logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "collect-strip",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:55.917063Z",
     "iopub.status.busy": "2021-04-12T07:59:55.916125Z",
     "iopub.status.idle": "2021-04-12T07:59:58.083145Z",
     "shell.execute_reply": "2021-04-12T07:59:58.082704Z"
    },
    "papermill": {
     "duration": 2.219359,
     "end_time": "2021-04-12T07:59:58.083266",
     "exception": false,
     "start_time": "2021-04-12T07:59:55.863907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check for data-dependant init in ActNorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 18:14:39.040896: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92mâœ“\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(m) \u001b[39m<\u001b[39m eps \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[91mx\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMean:\u001b[39m\u001b[39m\"\u001b[39m, m)\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92mâœ“\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(v  \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m eps \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[91mx\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mStandard deviation\u001b[39m\u001b[39m\"\u001b[39m, v)\n\u001b[0;32m---> 12\u001b[0m sanity_check()\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36msanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msanity_check\u001b[39m():\n\u001b[0;32m----> 4\u001b[0m     x \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mnormal(random_key, (\u001b[39m1\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m     model \u001b[39m=\u001b[39m ActNorm()\n\u001b[1;32m      6\u001b[0m     init_variables \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit(random_key, x)\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/random.py:562\u001b[0m, in \u001b[0;36mnormal\u001b[0;34m(key, shape, dtype)\u001b[0m\n\u001b[1;32m    560\u001b[0m dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(dtype)\n\u001b[1;32m    561\u001b[0m shape \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mas_named_shape(shape)\n\u001b[0;32m--> 562\u001b[0m \u001b[39mreturn\u001b[39;00m _normal(key, shape, dtype)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/interpreters/pxla.py:2136\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2134\u001b[0m   out_bufs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_tokens(input_bufs)\n\u001b[1;32m   2135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2136\u001b[0m   out_bufs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxla_executable\u001b[39m.\u001b[39;49mexecute_sharded_on_local_devices(\n\u001b[1;32m   2137\u001b[0m       input_bufs)\n\u001b[1;32m   2138\u001b[0m \u001b[39mif\u001b[39;00m dispatch\u001b[39m.\u001b[39mneeds_check_special():\n\u001b[1;32m   2139\u001b[0m   \u001b[39mfor\u001b[39;00m bufs \u001b[39min\u001b[39;00m out_bufs:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory."
     ]
    }
   ],
   "source": [
    "print(\"Sanity check for data-dependant init in ActNorm\")\n",
    "\n",
    "def sanity_check():\n",
    "    x = jax.random.normal(random_key, (1, 128, 128, 3))\n",
    "    model = ActNorm()\n",
    "    init_variables = model.init(random_key, x)\n",
    "    y, _ = model.apply(init_variables, x)\n",
    "    m = jnp.mean(y); v = jnp.std(y); eps = 1e-4\n",
    "    print(\"  \\033[92mâœ“\\033[0m\" if abs(m) < eps else \"  \\033[91mx\\033[0m\", \"Mean:\", m)\n",
    "    print(\"  \\033[92mâœ“\\033[0m\" if abs(v  - 1) < eps else \"  \\033[91mx\\033[0m\",\n",
    "          \"Standard deviation\", v)\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-airfare",
   "metadata": {
    "papermill": {
     "duration": 0.032572,
     "end_time": "2021-04-12T07:59:58.149583",
     "exception": false,
     "start_time": "2021-04-12T07:59:58.117011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Invertible Convolution\n",
    "\n",
    "\n",
    "\n",
    "**Forward**\n",
    "\\begin{align}\n",
    "y = W x\n",
    "\\end{align}\n",
    "\n",
    "**Backward**\n",
    "\\begin{align}\n",
    "x = W^{-1} y\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Log-det:**\n",
    "$\\log \\det \\mbox{ActNorm} = h \\times w \\times \\sum \\log (| \\det (W)|)$\n",
    "\n",
    "In order to make the determinant computation more efficient, the authors propose to work directly with the LU-decomposition of $W$ (*see original paper, section 3.2*), which is initialized as a rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-rental",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:58.255204Z",
     "iopub.status.busy": "2021-04-12T07:59:58.245813Z",
     "iopub.status.idle": "2021-04-12T07:59:58.258369Z",
     "shell.execute_reply": "2021-04-12T07:59:58.259973Z"
    },
    "papermill": {
     "duration": 0.07715,
     "end_time": "2021-04-12T07:59:58.260139",
     "exception": false,
     "start_time": "2021-04-12T07:59:58.182989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv1x1(nn.Module):\n",
    "    channels: int\n",
    "    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"Initialize P, L, U, s\"\"\"\n",
    "        # W = PL(U + s)\n",
    "        # Based on https://github.com/openai/glow/blob/master/model.py#L485\n",
    "        c = self.channels\n",
    "        # Sample random rotation matrix\n",
    "        q, _ = jnp.linalg.qr(jax.random.normal(self.key, (c, c)), mode='complete')\n",
    "        p, l, u = jax.scipy.linalg.lu(q)\n",
    "        # Fixed Permutation (non-trainable)\n",
    "        self.P = p\n",
    "        self.P_inv = jax.scipy.linalg.inv(p)\n",
    "        # Init value from LU decomposition\n",
    "        L_init = l\n",
    "        U_init = jnp.triu(u, k=1)\n",
    "        s = jnp.diag(u)\n",
    "        self.sign_s = jnp.sign(s)\n",
    "        S_log_init = jnp.log(jnp.abs(s))\n",
    "        self.l_mask = jnp.tril(jnp.ones((c, c)), k=-1)\n",
    "        self.u_mask = jnp.transpose(self.l_mask)\n",
    "        # Define trainable variables\n",
    "        self.L = self.param(\"L\", lambda k, sh: L_init, (c, c))\n",
    "        self.U = self.param(\"U\", lambda k, sh: U_init, (c, c))\n",
    "        self.log_s = self.param(\"log_s\", lambda k, sh: S_log_init, (c,))\n",
    "        \n",
    "        \n",
    "    def __call__(self, inputs, logdet=0, reverse=False):\n",
    "        c = self.channels\n",
    "        assert c == inputs.shape[-1]\n",
    "        # enforce constraints that L and U are triangular\n",
    "        # in the LU decomposition\n",
    "        L = self.L * self.l_mask + jnp.eye(c)\n",
    "        U = self.U * self.u_mask + jnp.diag(self.sign_s * jnp.exp(self.log_s))\n",
    "        logdet_factor = inputs.shape[1] * inputs.shape[2]\n",
    "        \n",
    "        # forward\n",
    "        if not reverse:\n",
    "            # lax.conv uses weird ordering: NCHW and OIHW\n",
    "            W = jnp.matmul(self.P, jnp.matmul(L, U))\n",
    "            y = jax.lax.conv(jnp.transpose(inputs, (0, 3, 1, 2)), \n",
    "                             W[..., None, None], (1, 1), 'same')\n",
    "            y = jnp.transpose(y, (0, 2, 3, 1))\n",
    "            logdet += jnp.sum(self.log_s) * logdet_factor\n",
    "        # inverse\n",
    "        else:\n",
    "            W_inv = jnp.matmul(jax.scipy.linalg.inv(U), jnp.matmul(\n",
    "                jax.scipy.linalg.inv(L), self.P_inv))\n",
    "            y = jax.lax.conv(jnp.transpose(inputs, (0, 3, 1, 2)),\n",
    "                             W_inv[..., None, None], (1, 1), 'same')\n",
    "            y = jnp.transpose(y, (0, 2, 3, 1))\n",
    "            logdet -= jnp.sum(self.log_s) * logdet_factor\n",
    "            \n",
    "        return y, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-workstation",
   "metadata": {
    "papermill": {
     "duration": 0.054953,
     "end_time": "2021-04-12T07:59:58.372860",
     "exception": false,
     "start_time": "2021-04-12T07:59:58.317907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Wrap-up: Normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-skill",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:58.490785Z",
     "iopub.status.busy": "2021-04-12T07:59:58.489968Z",
     "iopub.status.idle": "2021-04-12T07:59:58.508732Z",
     "shell.execute_reply": "2021-04-12T07:59:58.507687Z"
    },
    "papermill": {
     "duration": 0.083811,
     "end_time": "2021-04-12T07:59:58.508888",
     "exception": false,
     "start_time": "2021-04-12T07:59:58.425077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FlowStep(nn.Module):\n",
    "    nn_width: int = 512\n",
    "    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x, logdet=0, reverse=False):\n",
    "        out_dims = x.shape[-1]\n",
    "        if not reverse:\n",
    "            x, logdet = ActNorm()(x, logdet=logdet, reverse=False)\n",
    "            x, logdet = Conv1x1(out_dims, self.key)(x, logdet=logdet, reverse=False)\n",
    "            x, logdet = AffineCoupling(out_dims, self.nn_width)(x, logdet=logdet, reverse=False)\n",
    "        else:\n",
    "            x, logdet = AffineCoupling(out_dims, self.nn_width)(x, logdet=logdet, reverse=True)\n",
    "            x, logdet = Conv1x1(out_dims, self.key)(x, logdet=logdet, reverse=True)\n",
    "            x, logdet = ActNorm()(x, logdet=logdet, reverse=True)\n",
    "        return x, logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-warrant",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T07:59:58.642274Z",
     "iopub.status.busy": "2021-04-12T07:59:58.641355Z",
     "iopub.status.idle": "2021-04-12T08:00:08.884103Z",
     "shell.execute_reply": "2021-04-12T08:00:08.883610Z"
    },
    "papermill": {
     "duration": 10.317742,
     "end_time": "2021-04-12T08:00:08.884246",
     "exception": false,
     "start_time": "2021-04-12T07:59:58.566504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     init_variables \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit(random_key, x)\n\u001b[1;32m     35\u001b[0m     summarize_jax_model(init_variables, max_depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m summary()\n",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m, in \u001b[0;36msummary\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal(random_key, (\u001b[39m16\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[1;32m     33\u001b[0m model \u001b[39m=\u001b[39m FlowStep(key\u001b[39m=\u001b[39mrandom_key)\n\u001b[0;32m---> 34\u001b[0m init_variables \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minit(random_key, x)\n\u001b[1;32m     35\u001b[0m summarize_jax_model(init_variables, max_depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mFlowStep.__call__\u001b[0;34m(self, x, logdet, reverse)\u001b[0m\n\u001b[1;32m      9\u001b[0m     x, logdet \u001b[39m=\u001b[39m ActNorm()(x, logdet\u001b[39m=\u001b[39mlogdet, reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m     x, logdet \u001b[39m=\u001b[39m Conv1x1(out_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey)(x, logdet\u001b[39m=\u001b[39mlogdet, reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m     x, logdet \u001b[39m=\u001b[39m AffineCoupling(out_dims, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn_width)(x, logdet\u001b[39m=\u001b[39;49mlogdet, reverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     x, logdet \u001b[39m=\u001b[39m AffineCoupling(out_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_width)(x, logdet\u001b[39m=\u001b[39mlogdet, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mAffineCoupling.__call__\u001b[0;34m(self, inputs, logdet, reverse, dilation, only_neighbours)\u001b[0m\n\u001b[1;32m     43\u001b[0m mu, logsigma \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msplit(net, \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# mu and logsigma will be also the same dimension as the to-be-transformed\u001b[39;00m\n\u001b[1;32m     44\u001b[0m sigma \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msigmoid(logsigma \u001b[39m+\u001b[39m \u001b[39m2.\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39;49mexpand_dims(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m only_neighbours\u001b[39m==\u001b[39m\u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m \u001b[39m#mu and sigma (batch_size,1,8,8,3) ->  (batch_size,15,8,8,3)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m reverse:\n\u001b[1;32m     49\u001b[0m         \u001b[39m#get neighbours of the chosen patch\u001b[39;00m\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:921\u001b[0m, in \u001b[0;36mexpand_dims\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(a, \u001b[39m\"\u001b[39m\u001b[39mexpand_dims\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    920\u001b[0m   \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39mexpand_dims(axis)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39;49mexpand_dims(a, axis)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/util.py:352\u001b[0m, in \u001b[0;36mcanonicalize_axis\u001b[0;34m(axis, num_dims)\u001b[0m\n\u001b[1;32m    350\u001b[0m axis \u001b[39m=\u001b[39m operator\u001b[39m.\u001b[39mindex(axis)\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m-\u001b[39mnum_dims \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m axis \u001b[39m<\u001b[39m num_dims:\n\u001b[0;32m--> 352\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maxis \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m is out of bounds for array of dimension \u001b[39m\u001b[39m{\u001b[39;00mnum_dims\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    354\u001b[0m   axis \u001b[39m=\u001b[39m axis \u001b[39m+\u001b[39m num_dims\n",
      "\u001b[0;31mValueError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# Utils to display Jax model in a similar way as flax summary\n",
    "def get_params_size(v, s=0):\n",
    "    \"\"\"Get cumulative size of parameters contained in a FrozenDict\"\"\"\n",
    "    if isinstance(v, flax.core.FrozenDict):\n",
    "        return s + sum(get_params_size(x)  for x in v.values())\n",
    "    else:\n",
    "        return s + v.size\n",
    "\n",
    "def summarize_jax_model(variables, \n",
    "                        max_depth=1, \n",
    "                        depth=0,\n",
    "                        prefix='',\n",
    "                        col1_size=60,\n",
    "                        col2_size=30):\n",
    "    \"\"\"Print summary of parameters + size contained in a jax model\"\"\"\n",
    "    if depth == 0:\n",
    "        print('-' * (col1_size + col2_size))\n",
    "        print(\"Layer name\" + ' ' * (col1_size - 10) + 'Param #')\n",
    "        print('=' * (col1_size + col2_size))\n",
    "    for name, v in variables.items():\n",
    "        if isinstance(v, flax.core.FrozenDict) and depth < max_depth:\n",
    "            summarize_jax_model(v, max_depth=max_depth, depth=depth + 1, \n",
    "                                prefix=f'{prefix}/{name}')\n",
    "        else:\n",
    "            col1 = f'{prefix}/{name}'\n",
    "            col1 = col1[:col1_size] + ' ' * max(0, col1_size - len(col1))\n",
    "            print(f'{col1}{get_params_size(v)}')\n",
    "            print('-' * (col1_size + col2_size))\n",
    "            \n",
    "# Summarize a flow step\n",
    "def summary():\n",
    "    x = jax.random.normal(random_key, (16, 32, 32, 3))\n",
    "    model = FlowStep(key=random_key)\n",
    "    init_variables = model.init(random_key, x)\n",
    "    summarize_jax_model(init_variables, max_depth=2)\n",
    "summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-particle",
   "metadata": {
    "papermill": {
     "duration": 0.03513,
     "end_time": "2021-04-12T08:00:08.955903",
     "exception": false,
     "start_time": "2021-04-12T08:00:08.920773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Final model\n",
    "\n",
    "Once we have the flow step definition, we can finally buid the multi-scale Glow architecture. The naming of the different modules is important as it guarantees that the parameters are shared adequately between the forward and reverse pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-terrace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:09.046621Z",
     "iopub.status.busy": "2021-04-12T08:00:09.045480Z",
     "iopub.status.idle": "2021-04-12T08:00:09.051842Z",
     "shell.execute_reply": "2021-04-12T08:00:09.051405Z"
    },
    "papermill": {
     "duration": 0.060269,
     "end_time": "2021-04-12T08:00:09.051966",
     "exception": false,
     "start_time": "2021-04-12T08:00:08.991697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GLOW(nn.Module):\n",
    "    K: int = 48                                       # Number of flow steps\n",
    "    L: int = 1                                        # Number of scales\n",
    "    nn_width: int = 512                               # NN width in Affine Coupling Layer\n",
    "    learn_top_prior: bool = False                     # If true, learn prior N(mu, sigma) for zL\n",
    "    key: jax.random.PRNGKey = jax.random.PRNGKey(0)\n",
    "        \n",
    "        \n",
    "    def flows(self, x, logdet=0, reverse=False, name=\"\"):\n",
    "        \"\"\"K subsequent flows. Called at each scale.\"\"\"\n",
    "        for k in range(self.K):\n",
    "            it = k + 1 if not reverse else self.K - k\n",
    "            x, logdet = FlowStep(self.nn_width, self.key, name=f\"{name}/step_{it}\")(\n",
    "                x, logdet=logdet, reverse=reverse)\n",
    "        return x, logdet\n",
    "        \n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, reverse=False, z=None, eps=None, sampling_temperature=1.0):\n",
    "        \"\"\"Args:\n",
    "            * x: Input to the model\n",
    "            * reverse: Whether to apply the model or its inverse\n",
    "            * z (reverse = True): If given, use these as intermediate latents (deterministic)\n",
    "            * eps (reverse = True, z!=None): If given, use these as Gaussian samples which are later \n",
    "                rescaled by the mean and variance of the appropriate prior.\n",
    "            * sampling_temperature (reverse = True, z!=None): Sampling temperature\n",
    "        \"\"\"\n",
    "        \n",
    "        ## Inputs\n",
    "        # Forward pass: Save priors for computing loss\n",
    "        # Optionally save zs (only used for sanity check of reversibility)\n",
    "        priors = []\n",
    "        if not reverse:\n",
    "            del z, eps, sampling_temperature\n",
    "            z = []\n",
    "        # In reverse mode, either use the given latent z (deterministic)\n",
    "        # or sample them. For the first one, uses the top prior.\n",
    "        # The intermediate latents are sampled in the `Split(reverse=True)` calls\n",
    "        else:\n",
    "            if z is not None:\n",
    "                assert len(z) == self.L\n",
    "            else:\n",
    "                x *= sampling_temperature\n",
    "                if self.learn_top_prior:\n",
    "                    # Assumes input x is a sample from N(0, 1)\n",
    "                    # Note: the inputs to learn the top prior is zeros (unconditioned)\n",
    "                    # or some conditioning e.g. class information.\n",
    "                    # If not learnable, the model just uses the input x directly\n",
    "                    # see https://github.com/openai/glow/blob/master/model.py#L109\n",
    "                    prior = ConvZeros(x.shape[-1] * 2, name=\"prior_top\")(jnp.zeros(x.shape))\n",
    "                    mu, logsigma = jnp.split(prior, 2, axis=-1)\n",
    "                    x = x * jnp.exp(logsigma) + mu\n",
    "                \n",
    "        ## Multi-scale model\n",
    "        logdet = 0\n",
    "        for l in range(self.L):\n",
    "            # Forward\n",
    "            if not reverse:\n",
    "                x, logdet = self.flows(x, logdet=logdet,\n",
    "                                       reverse=False,\n",
    "                                       name=f\"flow_scale_{l + 1}/\")\n",
    "                if l < self.L - 1:\n",
    "                    zl, x, prior = Split(\n",
    "                        key=self.key, name=f\"flow_scale_{l + 1}/\")(x, reverse=False)\n",
    "                else:\n",
    "                    zl, prior = x, None\n",
    "                    if self.learn_top_prior:\n",
    "                        prior = ConvZeros(zl.shape[-1] * 2, name=\"prior_top\")(jnp.zeros(zl.shape))\n",
    "                z.append(zl)\n",
    "                priors.append(prior)\n",
    "                    \n",
    "            # Reverse\n",
    "            else:\n",
    "                if l > 0:\n",
    "                    x = Split(key=self.key, name=f\"flow_scale_{self.L - l}/\")(\n",
    "                        x, reverse=True, \n",
    "                        z=z[-l - 1] if z is not None else None,\n",
    "                        eps=eps[-l - 1] if eps is not None else None,\n",
    "                        temperature=sampling_temperature)\n",
    "                x, logdet = self.flows(x, logdet=logdet, reverse=True,\n",
    "                                       name=f\"flow_scale_{self.L - l}/\")\n",
    "                \n",
    "        ## Return\n",
    "        return x, z, logdet, priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atmospheric-vacuum",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:09.147985Z",
     "iopub.status.busy": "2021-04-12T08:00:09.147098Z",
     "iopub.status.idle": "2021-04-12T08:00:37.273411Z",
     "shell.execute_reply": "2021-04-12T08:00:37.272634Z"
    },
    "papermill": {
     "duration": 28.186382,
     "end_time": "2021-04-12T08:00:37.273539",
     "exception": false,
     "start_time": "2021-04-12T08:00:09.087157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check for reversibility (no sampling in reverse pass)\n",
      "  \u001b[92mâœ“\u001b[0m Forward pass output shape is (32, 32, 32, 3)\n",
      "  \u001b[92mâœ“\u001b[0m Check intermediate latents shape\n",
      "  \u001b[92mâœ“\u001b[0m Check intermediate priors shape\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(32, 15, 8, 8, 3), (32, 8, 8, 3)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/util.py:254\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/util.py:247\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 247\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/lax/lax.py:141\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m@cache\u001b[39m()\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_broadcast_shapes_cached\u001b[39m(\u001b[39m*\u001b[39mshapes: Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m--> 141\u001b[0m   \u001b[39mreturn\u001b[39;00m _broadcast_shapes_uncached(\u001b[39m*\u001b[39;49mshapes)\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/lax/lax.py:157\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(32, 15, 8, 8, 3), (32, 8, 8, 3)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     diff \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(jnp\u001b[39m.\u001b[39mabs(x_1 \u001b[39m-\u001b[39m x_3))\n\u001b[1;32m     41\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92mâœ“\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m diff \u001b[39m<\u001b[39m \u001b[39m1e-4\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[91mx\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     42\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDiff between x and Glow_r o Glow (x) = \u001b[39m\u001b[39m{\u001b[39;00mdiff\u001b[39m:\u001b[39;00m\u001b[39m.3e\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m sanity_check()\n",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m, in \u001b[0;36msanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92mâœ“\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m correct_latent_shapes \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[91mx\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mCheck intermediate priors shape\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Reverse the network without sampling\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m x_3, \u001b[39m*\u001b[39m_ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mapply(init_variables, z[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], z\u001b[39m=\u001b[39;49mz, reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     38\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[92mâœ“\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x_1\u001b[39m.\u001b[39mshape, x_3\u001b[39m.\u001b[39mshape) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[91mx\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m     39\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mReverse pass output shape = Original shape =\u001b[39m\u001b[39m\"\u001b[39m, x_1\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     40\u001b[0m diff \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(jnp\u001b[39m.\u001b[39mabs(x_1 \u001b[39m-\u001b[39m x_3))\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 80\u001b[0m, in \u001b[0;36mGLOW.__call__\u001b[0;34m(self, x, reverse, z, eps, sampling_temperature)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[39mif\u001b[39;00m l \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     75\u001b[0m             x \u001b[39m=\u001b[39m Split(key\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey, name\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mflow_scale_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL \u001b[39m-\u001b[39m l\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)(\n\u001b[1;32m     76\u001b[0m                 x, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[1;32m     77\u001b[0m                 z\u001b[39m=\u001b[39mz[\u001b[39m-\u001b[39ml \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m z \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     78\u001b[0m                 eps\u001b[39m=\u001b[39meps[\u001b[39m-\u001b[39ml \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m eps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m                 temperature\u001b[39m=\u001b[39msampling_temperature)\n\u001b[0;32m---> 80\u001b[0m         x, logdet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflows(x, logdet\u001b[39m=\u001b[39;49mlogdet, reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     81\u001b[0m                                name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mflow_scale_\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mL \u001b[39m-\u001b[39;49m l\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     83\u001b[0m \u001b[39m## Return\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m x, z, logdet, priors\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mGLOW.flows\u001b[0;34m(self, x, logdet, reverse, name)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK):\n\u001b[1;32m     12\u001b[0m     it \u001b[39m=\u001b[39m k \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m reverse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK \u001b[39m-\u001b[39m k\n\u001b[0;32m---> 13\u001b[0m     x, logdet \u001b[39m=\u001b[39m FlowStep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn_width, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey, name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m/step_\u001b[39;49m\u001b[39m{\u001b[39;49;00mit\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)(\n\u001b[1;32m     14\u001b[0m         x, logdet\u001b[39m=\u001b[39;49mlogdet, reverse\u001b[39m=\u001b[39;49mreverse)\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m x, logdet\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mFlowStep.__call__\u001b[0;34m(self, x, logdet, reverse)\u001b[0m\n\u001b[1;32m     11\u001b[0m     x, logdet \u001b[39m=\u001b[39m AffineCoupling(out_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_width)(x, logdet\u001b[39m=\u001b[39mlogdet, reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     x, logdet \u001b[39m=\u001b[39m AffineCoupling(out_dims, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnn_width)(x, logdet\u001b[39m=\u001b[39;49mlogdet, reverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     14\u001b[0m     x, logdet \u001b[39m=\u001b[39m Conv1x1(out_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey)(x, logdet\u001b[39m=\u001b[39mlogdet, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m     x, logdet \u001b[39m=\u001b[39m ActNorm()(x, logdet\u001b[39m=\u001b[39mlogdet, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 69\u001b[0m, in \u001b[0;36mAffineCoupling.__call__\u001b[0;34m(self, inputs, logdet, reverse, dilation, only_neighbours)\u001b[0m\n\u001b[1;32m     67\u001b[0m     y \u001b[39m=\u001b[39m all_patches\n\u001b[1;32m     68\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39m#TODO - Fix logic of reverse pass for neighbours only\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     yb \u001b[39m=\u001b[39m (x_rest \u001b[39m-\u001b[39;49m mu) \u001b[39m/\u001b[39m (sigma \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\n\u001b[1;32m     70\u001b[0m     logdet \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msum(jnp\u001b[39m.\u001b[39mlog(sigma), axis\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[1;32m     71\u001b[0m     y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39minsert(arr\u001b[39m=\u001b[39myb,obj\u001b[39m=\u001b[39mrandom_patch,values\u001b[39m=\u001b[39mx_chosen,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)   \n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4948\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4946\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m   4947\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m-> 4948\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   4949\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m   4950\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported operand type(s) for \u001b[39m\u001b[39m{\u001b[39;00mopchar\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4951\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:83\u001b[0m, in \u001b[0;36m_one_to_one_binop.<locals>.<lambda>\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     81\u001b[0m   fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x1, x2, \u001b[39m/\u001b[39m: lax_fn(\u001b[39m*\u001b[39m_promote_args_numeric(numpy_fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, x1, x2))\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m   fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x1, x2, \u001b[39m/\u001b[39m: lax_fn(\u001b[39m*\u001b[39m_promote_args(numpy_fn\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, x1, x2))\n\u001b[1;32m     84\u001b[0m fn\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mjax.numpy.\u001b[39m\u001b[39m{\u001b[39;00mnumpy_fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m fn \u001b[39m=\u001b[39m jit(fn, inline\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/numpy/util.py:365\u001b[0m, in \u001b[0;36m_promote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    363\u001b[0m _check_arraylike(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[1;32m    364\u001b[0m _check_no_float0s(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 365\u001b[0m \u001b[39mreturn\u001b[39;00m _promote_shapes(fun_name, \u001b[39m*\u001b[39;49m_promote_dtypes(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/numpy/util.py:258\u001b[0m, in \u001b[0;36m_promote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mjax_numpy_rank_promotion \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    257\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 258\u001b[0m result_rank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(lax\u001b[39m.\u001b[39;49mbroadcast_shapes(\u001b[39m*\u001b[39;49mshapes))\n\u001b[1;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (result_rank \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(shp)) \u001b[39m+\u001b[39m shp)\n\u001b[1;32m    260\u001b[0m         \u001b[39mfor\u001b[39;00m arg, shp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/conda3/envs/jax_310/lib/python3.10/site-packages/jax/_src/lax/lax.py:157\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    155\u001b[0m result_shape \u001b[39m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(32, 15, 8, 8, 3), (32, 8, 8, 3)]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Sanity check for reversibility (no sampling in reverse pass)\")\n",
    "\n",
    "def sanity_check():\n",
    "    # Input\n",
    "    x_1 = jax.random.normal(random_key, (32, 32, 32, 3))\n",
    "    K, L = 48, 1\n",
    "    model = GLOW(K=K, L=L, nn_width=512, key=random_key, learn_top_prior=True)\n",
    "    init_variables = model.init(random_key, x_1)\n",
    "\n",
    "    # Forward call\n",
    "    _, z, logdet, priors = model.apply(init_variables, x_1)\n",
    "\n",
    "    # Check output shape\n",
    "    expected_h = x_1.shape[1]\n",
    "    expected_c = x_1.shape[-1]\n",
    "    print(\"  \\033[92mâœ“\\033[0m\" if z[-1].shape[1] == expected_h and z[-1].shape[-1] == expected_c \n",
    "          else \"  \\033[91mx\\033[0m\",\n",
    "          \"Forward pass output shape is\", z[-1].shape)\n",
    "\n",
    "    # Check sizes of the intermediate latent\n",
    "    correct_latent_shapes = True\n",
    "    correct_prior_shapes = True\n",
    "    for i, (zi, priori) in enumerate(zip(z, priors)):\n",
    "        expected_h = x_1.shape[1]\n",
    "        expected_c = x_1.shape[-1]\n",
    "        if zi.shape[1] != expected_h or zi.shape[-1] != expected_c:\n",
    "            correct_latent_shapes = False\n",
    "        if priori.shape[1] != expected_h or priori.shape[-1] != 2*expected_c:\n",
    "            correct_prior_shapes = False\n",
    "    print(\"  \\033[92mâœ“\\033[0m\" if correct_latent_shapes else \"  \\033[91mx\\033[0m\",\n",
    "          \"Check intermediate latents shape\")\n",
    "    print(\"  \\033[92mâœ“\\033[0m\" if correct_latent_shapes else \"  \\033[91mx\\033[0m\",\n",
    "          \"Check intermediate priors shape\")\n",
    "\n",
    "    # Reverse the network without sampling\n",
    "    x_3, *_ = model.apply(init_variables, z[-1], z=z, reverse=True)\n",
    "\n",
    "    print(\"  \\033[92mâœ“\\033[0m\" if np.array_equal(x_1.shape, x_3.shape) else \"  \\033[91mx\\033[0m\", \n",
    "          \"Reverse pass output shape = Original shape =\", x_1.shape)\n",
    "    diff = jnp.mean(jnp.abs(x_1 - x_3))\n",
    "    print(\"  \\033[92mâœ“\\033[0m\" if diff < 1e-4 else \"  \\033[91mx\\033[0m\", \n",
    "          f\"Diff between x and Glow_r o Glow (x) = {diff:.3e}\")\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-copper",
   "metadata": {
    "papermill": {
     "duration": 0.034305,
     "end_time": "2021-04-12T08:00:37.342981",
     "exception": false,
     "start_time": "2021-04-12T08:00:37.308676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training the model\n",
    "\n",
    "## Training loss\n",
    "\n",
    "### Latent Log-likelihood\n",
    "Following equation (1), we now only need to compute the likelihood of the latent variables, $\\log p(z)$ term; The remaining loss term is computed by accumulating the log-determinant when passing through every block of the normalizing flow.\n",
    "\n",
    "Since each $p(z)$ is a Gaussian by definition, the corresponding likelihood is easy to estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-growth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:37.420299Z",
     "iopub.status.busy": "2021-04-12T08:00:37.419774Z",
     "iopub.status.idle": "2021-04-12T08:00:37.423966Z",
     "shell.execute_reply": "2021-04-12T08:00:37.423550Z"
    },
    "papermill": {
     "duration": 0.046074,
     "end_time": "2021-04-12T08:00:37.424075",
     "exception": false,
     "start_time": "2021-04-12T08:00:37.378001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.vmap\n",
    "def get_logpz(z, priors):\n",
    "    logpz = 0\n",
    "    for zi, priori in zip(z, priors):\n",
    "        if priori is None:\n",
    "            mu = jnp.zeros(zi.shape)\n",
    "            logsigma = jnp.zeros(zi.shape)\n",
    "        else:\n",
    "            mu, logsigma = jnp.split(priori, 2, axis=-1)\n",
    "        logpz += jnp.sum(- logsigma - 0.5 * jnp.log(2 * jnp.pi) \n",
    "                         - 0.5 * (zi - mu) ** 2 / jnp.exp(2 * logsigma))\n",
    "    return logpz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-entrance",
   "metadata": {
    "papermill": {
     "duration": 0.062678,
     "end_time": "2021-04-12T08:00:37.534499",
     "exception": false,
     "start_time": "2021-04-12T08:00:37.471821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Note on jax.vmap:** The `vmap` function decorator can be used to indicate that a function should be vectorized across a given axis of its inputs (default is first axis). This is very useful to model a function that can be parallelized across a batch, e.g. a loss function like here or metrics.\n",
    "\n",
    "### Dequantization\n",
    "\n",
    "In [A note on the evaluation of generative models](https://arxiv.org/pdf/1511.01844.pdf), the authors observe that typical generative models work with probability densities, considering images as continuous variables, even though images are typically discrete inputs in [0; 255]. A common technique to *dequantize* the data, is to add some small uniform noise to the input training images, which we can incorporate in the output pipeline.\n",
    "\n",
    "In the original Glow implementation, they also introduce a `num_bits` parameter which allows for further controlling the quantization level of the input images (8 = standard `uint8`, 0 = binary images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-procurement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:37.661637Z",
     "iopub.status.busy": "2021-04-12T08:00:37.660763Z",
     "iopub.status.idle": "2021-04-12T08:00:37.666168Z",
     "shell.execute_reply": "2021-04-12T08:00:37.667212Z"
    },
    "papermill": {
     "duration": 0.075107,
     "end_time": "2021-04-12T08:00:37.667428",
     "exception": false,
     "start_time": "2021-04-12T08:00:37.592321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_fn(image_path, num_bits=5, size=256, training=True):\n",
    "    \"\"\"Read image file, quantize and map to [-0.5, 0.5] range.\n",
    "    If num_bits = 8, there is no quantization effect.\"\"\"\n",
    "    image = tf.io.decode_jpeg(tf.io.read_file(image_path))\n",
    "    # Resize input image\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, (size, size))\n",
    "    image = tf.clip_by_value(image, 0., 255.)\n",
    "    # Discretize to the given number of bits\n",
    "    if num_bits < 8:\n",
    "        image = tf.floor(image / 2 ** (8 - num_bits))\n",
    "    # Send to [-1, 1]\n",
    "    num_bins = 2 ** num_bits\n",
    "    image = image / num_bins - 0.5\n",
    "    if training:\n",
    "        image = image + tf.random.uniform(tf.shape(image), 0, 1. / num_bins)\n",
    "    return image\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def postprocess(x, num_bits):\n",
    "    \"\"\"Map [-0.5, 0.5] quantized images to uint space\"\"\"\n",
    "    num_bins = 2 ** num_bits\n",
    "    x = jnp.floor((x + 0.5) * num_bins)\n",
    "    x *= 256. / num_bins\n",
    "    return jnp.clip(x, 0, 255).astype(jnp.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-canadian",
   "metadata": {
    "papermill": {
     "duration": 0.058209,
     "end_time": "2021-04-12T08:00:37.785481",
     "exception": false,
     "start_time": "2021-04-12T08:00:37.727272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Note on jax.jit**: The `jit` decorator is essentially an optimization that compiles a block of operations acting on the same device together. See also the [jax doc](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)\n",
    "\n",
    "## Sampling\n",
    "\n",
    "Drawing a parallel to the standard VAE, we can see the *encoder* as a forward pass of the Glow module, and the *decoder* as a reverse pass (Glow$^{-1}$).  However, due to the reversible nature of the network, we do not actually need the reverse pass to compute the exact training objective, $p(x)$ as it depends only on the prior $p(z)$, and from the log-determinants of the normalizing flows leading from $x$ to $z$.\n",
    "\n",
    "In other words, we only need the encoder for the training phase. The \"decoder\" (i.e., reverse Glow) is used for sampling only. A sampling pass is thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-visiting",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:37.914685Z",
     "iopub.status.busy": "2021-04-12T08:00:37.913855Z",
     "iopub.status.idle": "2021-04-12T08:00:37.936242Z",
     "shell.execute_reply": "2021-04-12T08:00:37.937190Z"
    },
    "papermill": {
     "duration": 0.093191,
     "end_time": "2021-04-12T08:00:37.937406",
     "exception": false,
     "start_time": "2021-04-12T08:00:37.844215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(model, \n",
    "           params, \n",
    "           eps=None, \n",
    "           shape=None, \n",
    "           sampling_temperature=1.0, \n",
    "           key=jax.random.PRNGKey(0),\n",
    "           postprocess_fn=None, \n",
    "           save_path=None,\n",
    "           display=True):\n",
    "    \"\"\"Sampling only requires a call to the reverse pass of the model\"\"\"\n",
    "    if eps is None:\n",
    "        zL = jax.random.normal(key, shape) \n",
    "    else: \n",
    "        zL = eps[-1]\n",
    "    y, *_ = model.apply(params, zL, eps=eps, sampling_temperature=sampling_temperature, reverse=True)\n",
    "    if postprocess_fn is not None:\n",
    "        y = postprocess_fn(y)\n",
    "    plot_image_grid(y, save_path=save_path, display=display,\n",
    "                    title=None if save_path is None else save_path.rsplit('.', 1)[0].rsplit('/', 1)[1])\n",
    "    return y\n",
    "\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "def plot_image_grid(y, title=None, display=True, save_path=None, figsize=(10, 10)):\n",
    "    \"\"\"Plot and optionally save an image grid with matplotlib\"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    num_rows = int(np.floor(np.sqrt(y.shape[0])))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_rows), axes_pad=0.1)\n",
    "    for ax in grid: \n",
    "        ax.set_axis_off()\n",
    "    for ax, im in zip(grid, y):\n",
    "        ax.imshow(im)\n",
    "    fig.suptitle(title, fontsize=18)\n",
    "    fig.subplots_adjust(top=0.98)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    if display:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-thought",
   "metadata": {
    "papermill": {
     "duration": 0.047292,
     "end_time": "2021-04-12T08:00:38.044841",
     "exception": false,
     "start_time": "2021-04-12T08:00:37.997549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-providence",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:38.144987Z",
     "iopub.status.busy": "2021-04-12T08:00:38.143697Z",
     "iopub.status.idle": "2021-04-12T08:00:38.148071Z",
     "shell.execute_reply": "2021-04-12T08:00:38.147580Z"
    },
    "papermill": {
     "duration": 0.066409,
     "end_time": "2021-04-12T08:00:38.148201",
     "exception": false,
     "start_time": "2021-04-12T08:00:38.081792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_glow(train_ds,\n",
    "               val_ds=None,\n",
    "               num_samples=9,\n",
    "               image_size=256,\n",
    "               num_channels=3,\n",
    "               num_bits=5,\n",
    "               init_lr=1e-3,\n",
    "               num_epochs=1,\n",
    "               num_sample_epochs=1,\n",
    "               num_warmup_epochs=10,\n",
    "               num_save_epochs=1,\n",
    "               steps_per_epoch=1,\n",
    "               K=32,\n",
    "               L=3,\n",
    "               nn_width=512,\n",
    "               sampling_temperature=0.7,\n",
    "               learn_top_prior=True,\n",
    "               key=jax.random.PRNGKey(0),\n",
    "               **kwargs):\n",
    "    \"\"\"Simple training loop.\n",
    "    Args:\n",
    "        train_ds: Training dataset iterator (e.g. tensorflow dataset)\n",
    "        val_ds: Validation dataset (optional)\n",
    "        num_samples: Number of samples to generate at each epoch\n",
    "        image_size: Input image size\n",
    "        num_channels: Number of channels in input images\n",
    "        num_bits: Number of bits for discretization\n",
    "        init_lr: Initial learning rate (Adam)\n",
    "        num_epochs: Numer of training epochs\n",
    "        num_sample_epochs: Visualize sample at this interval\n",
    "        num_warmup_epochs: Linear warmup of the learning rate to init_lr\n",
    "        num_save_epochs: save mode at this interval\n",
    "        steps_per_epochs: Number of steps per epochs\n",
    "        K: Number of flow iterations in the GLOW model\n",
    "        L: number of scales in the GLOW model\n",
    "        nn_width: Layer width in the Affine Coupling Layer\n",
    "        sampling_temperature: Smoothing temperature for sampling from the \n",
    "            Gaussian priors (1 = no effect)\n",
    "        learn_top_prior: Whether to learn the prior for highest latent variable zL.\n",
    "            Otherwise, assumes standard unit Gaussian prior\n",
    "        key: Random seed\n",
    "    \"\"\"\n",
    "    del kwargs\n",
    "    # Init model\n",
    "    model = GLOW(K=K,\n",
    "                 L=L, \n",
    "                 nn_width=nn_width, \n",
    "                 learn_top_prior=learn_top_prior,\n",
    "                 key=key)\n",
    "    \n",
    "    # Init optimizer and learning rate schedule\n",
    "    params = model.init(random_key, next(train_ds))\n",
    "    opt = flax.optim.Adam(learning_rate=init_lr).create(params)\n",
    "    \n",
    "    def lr_warmup(step):\n",
    "        return init_lr * jnp.minimum(1., step / (num_warmup_epochs * steps_per_epoch + 1e-8))\n",
    "    \n",
    "    # Helper functions for training\n",
    "    bits_per_dims_norm = np.log(2.) * num_channels * image_size**2\n",
    "    @jax.jit\n",
    "    def get_logpx(z, logdets, priors):\n",
    "        logpz = get_logpz(z, priors)\n",
    "        logpz = jnp.mean(logpz) / bits_per_dims_norm        # bits per dimension normalization\n",
    "        logdets = jnp.mean(logdets) / bits_per_dims_norm\n",
    "        logpx = logpz + logdets - num_bits                  # num_bits: dequantization factor\n",
    "        return logpx, logpz, logdets\n",
    "        \n",
    "    @jax.jit\n",
    "    def train_step(opt, batch):\n",
    "        def loss_fn(params):\n",
    "            _, z, logdets, priors = model.apply(params, batch, reverse=False)\n",
    "            logpx, logpz, logdets = get_logpx(z, logdets, priors)\n",
    "            return - logpx, (logpz, logdets)\n",
    "        logs, grad = jax.value_and_grad(loss_fn, has_aux=True)(opt.target)\n",
    "        opt = opt.apply_gradient(grad, learning_rate=lr_warmup(opt.state.step))\n",
    "        return logs, opt\n",
    "    \n",
    "    # Helper functions for evaluation \n",
    "    @jax.jit\n",
    "    def eval_step(params, batch):\n",
    "        _, z, logdets, priors = model.apply(params, batch, reverse=False)\n",
    "        return - get_logpx(z, logdets, priors)[0]\n",
    "    \n",
    "    # Helper function for sampling from random latent fixed during training for comparison\n",
    "    eps = []\n",
    "    if not os.path.exists(\"samples\"): os.makedirs(\"samples\")\n",
    "    if not os.path.exists(\"weights\"): os.makedirs(\"weights\")\n",
    "    for i in range(L):\n",
    "        expected_h = image_size\n",
    "        expected_c = num_channels\n",
    "        eps.append(jax.random.normal(key, (num_samples, expected_h, expected_h, expected_c)))\n",
    "    sample_fn = partial(sample, eps=eps, key=key, display=False,\n",
    "                        sampling_temperature=sampling_temperature,\n",
    "                        postprocess_fn=partial(postprocess, num_bits=num_bits))\n",
    "    \n",
    "    # Train\n",
    "    print(\"Start training...\")\n",
    "    print(\"Available jax devices:\", jax.devices())\n",
    "    print()\n",
    "    bits = 0.\n",
    "    start = time.time()\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # train\n",
    "            for i in range(steps_per_epoch):\n",
    "                batch = next(train_ds)\n",
    "                loss, opt = train_step(opt, batch)\n",
    "                print(f\"\\r\\033[92m[Epoch {epoch + 1}/{num_epochs}]\\033[0m\"\n",
    "                      f\"\\033[93m[Batch {i + 1}/{steps_per_epoch}]\\033[0m\"\n",
    "                      f\" loss = {loss[0]:.5f},\"\n",
    "                      f\" (log(p(z)) = {loss[1][0]:.5f},\"\n",
    "                      f\" logdet = {loss[1][1]:.5f})\", end='')\n",
    "                if np.isnan(loss[0]):\n",
    "                    print(\"\\nModel diverged - NaN loss\")\n",
    "                    return None, None\n",
    "                \n",
    "                step = epoch * steps_per_epoch + i + 1\n",
    "                if step % int(num_sample_epochs * steps_per_epoch) == 0:\n",
    "                    sample_fn(model, opt.target, \n",
    "                              save_path=f\"samples/step_{step:05d}.png\")\n",
    "\n",
    "            # eval on one batch of validation samples \n",
    "            # + generate random sample\n",
    "            t = time.time() - start\n",
    "            if val_ds is not None:\n",
    "                bits = eval_step(opt.target, next(val_ds))\n",
    "            print(f\"\\r\\033[92m[Epoch {epoch + 1}/{num_epochs}]\\033[0m\"\n",
    "                  f\"[{int(t // 3600):02d}h {int((t % 3600) // 60):02d}mn]\"\n",
    "                  f\" train_bits/dims = {loss[0]:.3f},\"\n",
    "                  f\" val_bits/dims = {bits:.3f}\" + \" \" * 50)\n",
    "            \n",
    "            # Save parameters\n",
    "            if (epoch + 1) % num_save_epochs == 0 or epoch == num_epochs - 1:\n",
    "                with open(f'weights/model_epoch={epoch + 1:03d}.weights', 'wb') as f:\n",
    "                    f.write(flax.serialization.to_bytes(opt.target))\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\nInterrupted by user at epoch {epoch + 1}\")\n",
    "        \n",
    "    # returns final model and parameters\n",
    "    return model, opt.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-happiness",
   "metadata": {
    "papermill": {
     "duration": 0.035548,
     "end_time": "2021-04-12T08:00:38.219427",
     "exception": false,
     "start_time": "2021-04-12T08:00:38.183879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Note on multi-devices training:** To extend the code for training on multi-devices we can make use of the `jax.vmap` operator (parallelize across XLA devices) instead of `jax.jit`, we also need to share the current parameters with all the devices (use `flax.jax_utils.replicate` on the optimizer before training), and finally to split the data across the devices, which can be handled with the `tf.data` in the input pipeline. [There is a more complete tutorial with an example here](https://flax.readthedocs.io/en/stable/howtos/ensembling.html)\n",
    "\n",
    "# Experiments\n",
    "\n",
    "**Note:** We run for 12 epochs due to time limits (roughly 40k training steps) + smaller flow depth (*K*) to fit into single GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-adelaide",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:38.296525Z",
     "iopub.status.busy": "2021-04-12T08:00:38.295903Z",
     "iopub.status.idle": "2021-04-12T08:00:38.300025Z",
     "shell.execute_reply": "2021-04-12T08:00:38.299597Z"
    },
    "papermill": {
     "duration": 0.045839,
     "end_time": "2021-04-12T08:00:38.300130",
     "exception": false,
     "start_time": "2021-04-12T08:00:38.254291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data hyperparameters for 1 GPU training\n",
    "# Some small changes to the original model so \n",
    "# everything fits in memory\n",
    "# In particular, I had  to use shallower\n",
    "# flows (smaller K value)\n",
    "config_dict = {\n",
    "    'image_path': \"../input/celeba-dataset/img_align_celeba/img_align_celeba\",\n",
    "    'train_split': 0.6,\n",
    "    'image_size': 64,\n",
    "    'num_channels': 3,\n",
    "    'num_bits': 5,\n",
    "    'batch_size': 64,\n",
    "    'K': 16,\n",
    "    'L': 3,\n",
    "    'nn_width': 512, \n",
    "    'learn_top_prior': True,\n",
    "    'sampling_temperature': 0.7,\n",
    "    'init_lr': 1e-3,\n",
    "    'num_epochs': 13,\n",
    "    'num_warmup_epochs': 1,\n",
    "    'num_sample_epochs': 0.2, # Fractional epochs for sampling because one epoch is quite long \n",
    "    'num_save_epochs': 5,\n",
    "}\n",
    "\n",
    "output_hw = config_dict[\"image_size\"] // 2 ** config_dict[\"L\"]\n",
    "output_c = config_dict[\"num_channels\"] * 4**config_dict[\"L\"] // 2**(config_dict[\"L\"] - 1)\n",
    "config_dict[\"sampling_shape\"] = (output_hw, output_hw, output_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-accent",
   "metadata": {
    "papermill": {
     "duration": 0.034909,
     "end_time": "2021-04-12T08:00:38.369926",
     "exception": false,
     "start_time": "2021-04-12T08:00:38.335017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-tourism",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:38.450229Z",
     "iopub.status.busy": "2021-04-12T08:00:38.449717Z",
     "iopub.status.idle": "2021-04-12T08:00:42.747434Z",
     "shell.execute_reply": "2021-04-12T08:00:42.746356Z"
    },
    "papermill": {
     "duration": 4.341473,
     "end_time": "2021-04-12T08:00:42.747579",
     "exception": false,
     "start_time": "2021-04-12T08:00:38.406106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "\n",
    "def get_train_dataset(image_path, image_size, num_bits, batch_size, skip=None, **kwargs):\n",
    "    del kwargs\n",
    "    train_ds = tf.data.Dataset.list_files(f\"{image_path}/*.jpg\")\n",
    "    if skip is not None:\n",
    "        train_ds = train_ds.skip(skip)\n",
    "    train_ds = train_ds.shuffle(buffer_size=20000)\n",
    "    train_ds = train_ds.map(partial(map_fn, size=image_size, num_bits=num_bits, training=True))\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = train_ds.repeat()\n",
    "    return iter(tfds.as_numpy(train_ds))\n",
    "\n",
    "\n",
    "def get_val_dataset(image_path, image_size, num_bits, batch_size, \n",
    "                    take=None, repeat=False, **kwargs):\n",
    "    del kwargs\n",
    "    val_ds = tf.data.Dataset.list_files(f\"{image_path}/*.jpg\")\n",
    "    if take is not None:\n",
    "        val_ds = val_ds.take(take)\n",
    "    val_ds = val_ds.map(partial(map_fn, size=image_size, num_bits=num_bits, training=False))\n",
    "    val_ds = val_ds.batch(batch_size)\n",
    "    if repeat:\n",
    "        val_ds = val_ds.repeat()\n",
    "    return iter(tfds.as_numpy(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-horror",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T08:00:42.826328Z",
     "iopub.status.busy": "2021-04-12T08:00:42.825817Z",
     "iopub.status.idle": "2021-04-12T08:04:23.819752Z",
     "shell.execute_reply": "2021-04-12T08:04:23.819259Z"
    },
    "papermill": {
     "duration": 221.035393,
     "end_time": "2021-04-12T08:04:23.819876",
     "exception": false,
     "start_time": "2021-04-12T08:00:42.784483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_images = len(glob.glob(f\"{config_dict['image_path']}/*.jpg\"))\n",
    "config_dict['steps_per_epoch'] = num_images // config_dict['batch_size']\n",
    "train_split = int(config_dict['train_split'] * num_images)\n",
    "print(f\"{num_images} training images\")\n",
    "print(f\"{config_dict['steps_per_epoch']} training steps per epoch\")\n",
    "\n",
    "#Train data\n",
    "train_ds = get_train_dataset(**config_dict, skip=train_split)\n",
    "\n",
    "# Val data\n",
    "# During training we'll only evaluate on one batch of validation \n",
    "# to save on computations\n",
    "val_ds = get_val_dataset(**config_dict, take=config_dict['batch_size'], repeat=True)\n",
    "\n",
    "# Sample\n",
    "plot_image_grid(postprocess(next(val_ds), num_bits=config_dict['num_bits'])[:25], \n",
    "                title=\"Input data sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-administration",
   "metadata": {
    "papermill": {
     "duration": 0.050374,
     "end_time": "2021-04-12T08:04:23.921997",
     "exception": false,
     "start_time": "2021-04-12T08:04:23.871623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-exclusive",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T08:04:24.028734Z",
     "iopub.status.busy": "2021-04-12T08:04:24.027676Z",
     "iopub.status.idle": "2021-04-12T15:39:55.159329Z",
     "shell.execute_reply": "2021-04-12T15:39:55.158849Z"
    },
    "papermill": {
     "duration": 27331.188753,
     "end_time": "2021-04-12T15:39:55.161143",
     "exception": false,
     "start_time": "2021-04-12T08:04:23.972390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, params = train_glow(train_ds, val_ds=val_ds, **config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-grace",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:40:17.509218Z",
     "iopub.status.busy": "2021-04-12T15:40:17.487573Z",
     "iopub.status.idle": "2021-04-12T15:40:19.576687Z",
     "shell.execute_reply": "2021-04-12T15:40:19.577117Z"
    },
    "papermill": {
     "duration": 13.307516,
     "end_time": "2021-04-12T15:40:19.577280",
     "exception": false,
     "start_time": "2021-04-12T15:40:06.269764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Random samples evolution during training\")\n",
    "from PIL import Image\n",
    "\n",
    "# filepaths\n",
    "fp_in = \"samples/step_*.png\"\n",
    "fp_out = \"sample_evolution.gif\"\n",
    "\n",
    "# https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif\n",
    "img, *imgs = [Image.open(f) for f in sorted(glob.glob(fp_in))]\n",
    "img.save(fp=fp_out, format='GIF', append_images=imgs,\n",
    "         save_all=True, duration=200, loop=0)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<img src=\"sample_evolution.gif\">'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-terminology",
   "metadata": {
    "papermill": {
     "duration": 11.044445,
     "end_time": "2021-04-12T15:40:42.182704",
     "exception": false,
     "start_time": "2021-04-12T15:40:31.138259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-champagne",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T15:41:04.692240Z",
     "iopub.status.busy": "2021-04-12T15:41:04.691691Z",
     "iopub.status.idle": "2021-04-12T15:41:04.696459Z",
     "shell.execute_reply": "2021-04-12T15:41:04.696011Z"
    },
    "papermill": {
     "duration": 11.601025,
     "end_time": "2021-04-12T15:41:04.696574",
     "exception": false,
     "start_time": "2021-04-12T15:40:53.095549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional, example code to load trained weights\n",
    "if False:\n",
    "    model = GLOW(K=config_dict['K'],\n",
    "                 L=config_dict['L'], \n",
    "                 nn_width=config_dict['nn_width'], \n",
    "                 learn_top_prior=config_dict['learn_top_prior'])\n",
    "\n",
    "    with open('weights/model_epoch=100.weights', 'rb') as f:\n",
    "        params = model.init(random_key, jnp.zeros((config_dict['batch_size'],\n",
    "                                                     config_dict['image_size'],\n",
    "                                                     config_dict['image_size'],\n",
    "                                                     config_dict['num_channels'])))\n",
    "        params = flax.serialization.from_bytes(params, f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-sellers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-12T15:41:26.786697Z",
     "iopub.status.busy": "2021-04-12T15:41:26.784842Z",
     "iopub.status.idle": "2021-04-12T15:41:26.787285Z",
     "shell.execute_reply": "2021-04-12T15:41:26.787698Z"
    },
    "papermill": {
     "duration": 10.929332,
     "end_time": "2021-04-12T15:41:26.787834",
     "exception": false,
     "start_time": "2021-04-12T15:41:15.858502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct(model, params, batch):\n",
    "    global config_dict\n",
    "    x, z, logdets, priors = model.apply(params, batch, reverse=False)\n",
    "    rec, *_ = model.apply(params, z[-1], z=z, reverse=True)\n",
    "    rec = postprocess(rec, config_dict[\"num_bits\"])\n",
    "    plot_image_grid(postprocess(batch, config_dict[\"num_bits\"]), title=\"original\")\n",
    "    plot_image_grid(rec, title=\"reconstructions\")\n",
    "    \n",
    "\n",
    "def interpolate(model, params, batch, num_samples=16):\n",
    "    global config_dict\n",
    "    i1, i2 = np.random.choice(range(batch.shape[0]), size=2, replace=False)\n",
    "    in_ = np.stack([batch[i1], batch[i2]], axis=0)\n",
    "    x, z, logdets, priors = model.apply(params, in_, reverse=False)\n",
    "    # interpolate\n",
    "    interpolated_z = []\n",
    "    for zi in z:\n",
    "        z_1, z_2 = zi[:2]\n",
    "        interpolate = jnp.array([t * z_1 + (1 - t) * z_2 for t in np.linspace(0., 1., 16)])\n",
    "        interpolated_z.append(interpolate)\n",
    "    rec, *_ = model.apply(params, interpolated_z[-1], z=interpolated_z, reverse=True)\n",
    "    rec = postprocess(rec, config_dict[\"num_bits\"])\n",
    "    plot_image_grid(rec, title=\"Linear interpolation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-grammar",
   "metadata": {
    "papermill": {
     "duration": 11.020073,
     "end_time": "2021-04-12T15:41:49.314696",
     "exception": false,
     "start_time": "2021-04-12T15:41:38.294623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reconstructions\n",
    "As a sanity check let's first look at image reconstructions: since the model is invertible these should always be perfect, up to small float errors, except in very bad cases e.g. NaN values or other numerical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-gilbert",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:42:11.633001Z",
     "iopub.status.busy": "2021-04-12T15:42:11.631913Z",
     "iopub.status.idle": "2021-04-12T15:42:35.692850Z",
     "shell.execute_reply": "2021-04-12T15:42:35.693257Z"
    },
    "papermill": {
     "duration": 35.522545,
     "end_time": "2021-04-12T15:42:35.693418",
     "exception": false,
     "start_time": "2021-04-12T15:42:00.170873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(val_ds)\n",
    "reconstruct(model, params, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-makeup",
   "metadata": {
    "papermill": {
     "duration": 11.034127,
     "end_time": "2021-04-12T15:42:58.315209",
     "exception": false,
     "start_time": "2021-04-12T15:42:47.281082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sampling\n",
    "Now let's take some random samples from the model, starting from the learned priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-demand",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:43:21.066831Z",
     "iopub.status.busy": "2021-04-12T15:43:21.052362Z",
     "iopub.status.idle": "2021-04-12T15:43:34.220551Z",
     "shell.execute_reply": "2021-04-12T15:43:34.221280Z"
    },
    "papermill": {
     "duration": 24.878295,
     "end_time": "2021-04-12T15:43:34.221512",
     "exception": false,
     "start_time": "2021-04-12T15:43:09.343217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"],  key=random_key,\n",
    "       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n",
    "       save_path=\"samples/final_random_sample_T=1.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-penny",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:43:56.835211Z",
     "iopub.status.busy": "2021-04-12T15:43:56.834215Z",
     "iopub.status.idle": "2021-04-12T15:44:01.855209Z",
     "shell.execute_reply": "2021-04-12T15:44:01.855708Z"
    },
    "papermill": {
     "duration": 16.114872,
     "end_time": "2021-04-12T15:44:01.855880",
     "exception": false,
     "start_time": "2021-04-12T15:43:45.741008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"], \n",
    "       key=jax.random.PRNGKey(1), sampling_temperature=0.7,\n",
    "       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n",
    "       save_path=\"samples/final_random_sample_T=0.7.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-quality",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:44:24.635263Z",
     "iopub.status.busy": "2021-04-12T15:44:24.634313Z",
     "iopub.status.idle": "2021-04-12T15:44:30.377588Z",
     "shell.execute_reply": "2021-04-12T15:44:30.378026Z"
    },
    "papermill": {
     "duration": 17.285687,
     "end_time": "2021-04-12T15:44:30.378182",
     "exception": false,
     "start_time": "2021-04-12T15:44:13.092495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"], \n",
    "       key=jax.random.PRNGKey(2), sampling_temperature=0.7,\n",
    "       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n",
    "       save_path=\"samples/final_random_sample_T=0.7.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-revelation",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:44:53.199929Z",
     "iopub.status.busy": "2021-04-12T15:44:53.198719Z",
     "iopub.status.idle": "2021-04-12T15:44:58.692393Z",
     "shell.execute_reply": "2021-04-12T15:44:58.692847Z"
    },
    "papermill": {
     "duration": 17.088802,
     "end_time": "2021-04-12T15:44:58.692989",
     "exception": false,
     "start_time": "2021-04-12T15:44:41.604187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample(model, params, shape=(16,) + config_dict[\"sampling_shape\"], \n",
    "       key=jax.random.PRNGKey(3), sampling_temperature=0.5,\n",
    "       postprocess_fn=partial(postprocess, num_bits=config_dict[\"num_bits\"]),\n",
    "       save_path=\"samples/final_random_sample_T=0.5.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-fraud",
   "metadata": {
    "papermill": {
     "duration": 11.667376,
     "end_time": "2021-04-12T15:45:21.581232",
     "exception": false,
     "start_time": "2021-04-12T15:45:09.913856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Latent space\n",
    "Finally, we can look at the linear interpolation in the learned latent space: We generate embedding $z_1$ and $z_2$ by feeding two validation set images to Glow. Then we plot the decoded images for latent vectors $t + z_1 + (1 - t) z_2$ for $t \\in [0, 1]$ (at all level of the latent hierarchy).\n",
    "\n",
    "**Note on conditional modeling**  The model can also be extented to conditional generation (in the original code this is done by (i) learning the top prior from one-hot class embedding rather than all zeros input, and (ii) adding a small classifier on top of the output latent which should aim at predicting the correct class).\n",
    "\n",
    "In the original paper, this allows them to do \"semantic manipulation\" on the Celeba dataset by building representative centroid vectors for different attributes/classes (e.g.g $z_{smiling}$ and $z_{non-smiling}$). They can use then use the vector direction $z_{smiling}$ - $z_{non-smiling}$ as a guide to browse the latent space (in that example, to make images more or less \"smiling\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-alliance",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:45:43.843367Z",
     "iopub.status.busy": "2021-04-12T15:45:43.842441Z",
     "iopub.status.idle": "2021-04-12T15:46:01.569716Z",
     "shell.execute_reply": "2021-04-12T15:46:01.570167Z"
    },
    "papermill": {
     "duration": 28.882994,
     "end_time": "2021-04-12T15:46:01.570316",
     "exception": false,
     "start_time": "2021-04-12T15:45:32.687322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpolate(model, params, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-anxiety",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-04-12T15:46:23.837424Z",
     "iopub.status.busy": "2021-04-12T15:46:23.836605Z",
     "iopub.status.idle": "2021-04-12T15:46:31.996989Z",
     "shell.execute_reply": "2021-04-12T15:46:31.996494Z"
    },
    "papermill": {
     "duration": 19.295457,
     "end_time": "2021-04-12T15:46:31.997107",
     "exception": false,
     "start_time": "2021-04-12T15:46:12.701650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpolate(model, params, batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28036.31319,
   "end_time": "2021-04-12T15:46:48.892318",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-12T07:59:32.579128",
   "version": "2.3.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0acd1efabb006334362d5539cf634a5caec85a558927fcfefa2918e6e757f5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
